---
layout: post
title:  "CTPN论文的阅读与学习"
date:   2019-06-02 12:00:00 +0800
categories: cv
tags: CTPN
author: ac酱
mathjax: true
---

* content
{:toc}
最近在项目中使用了CTPN及CRNN进行中文OCR系统的构造，因此很有必要对这两个网络进行更进一步的了解与学习。本文将记录与CTPN相关的内容。



## CTPN流程概括

<center>
<img src="https://raw.githubusercontent.com/changwh/changwh.github.io/master/_posts/res/2019-06-02-ctpn-paper-reading/architecture.jpg" />
<div>CTPN整体结构</div>
</center>

<center>
<img src="https://raw.githubusercontent.com/changwh/changwh.github.io/master/_posts/res/2019-06-02-ctpn-paper-reading/model_code.jpg" />
<div>代码实现</div>
</center>


* 以VGG16为前置网络，用于输入图片的特征提取，使用经过其最后一个卷积层（VGG论文中conv3-512）得到的特征图，大小为W\*H\*C
* 在特征图的每个像素点上使用3\*3\*C的滑动窗口提取进一步特征，用于RPN
* 每一行像素通过滑动窗口处理后得到的特征（W\*3\*3\*C）作为256维BLSTM（包含两个128维的LSTM）的输入
* 将BLSTM的输出作为512维全连接层的输入
* 全连接层之后紧接着输出层，输出层包含三个分类或是回归。第二个2k scores表示的是k个anchor的类别信息（是否是字符）。第一个2k vertical coordinates（表示bounding box的高度和中心的y轴坐标，可决定上下边界）和第三个k side-refinement（表示bounding box的水平平移量，但在这里默认的宽度是固定的16像素）用来回归k个anchor的位置信息
* 通过简单的文本线构造算法，把分类得到的文字的proposal合并成文本线

## 关键的IDEA

### 锚点机制

CTPN通过在卷积特征图中密集地滑动小窗口来检测文本行，并且输出一系列细粒度（如宽度固定为16像素）的文本提议。当前置网络为VGG16时，滑动窗口的总步长和感受野分别为16（经过4次池化）和228像素（相当于VGG16最后一层之后又进行一次3\*3，stride=1的卷积，参见[感受野的计算](https://www.cnblogs.com/objectDetect/p/5947169.html)）。

通常滑动窗口采用多尺度窗口来检测不同尺寸的目标，其中一个窗口尺度被固定到与目标的尺寸相似。而锚点回归机制，允许RPN使用单尺度窗口检测多尺度目标。关键的见解是单个窗口能够通过使用多个灵活的锚点来预测各种尺度和长宽比的目标。

但是文本与普通目标不同，没有一个明显封闭的边界，不能从一部分推断整个目标。因为文本中的每个字符都是独立或分离的，所以查找文本的开始和结束位置十分困难。但是文本又是一个序列，因此可以很自然地将其视为一系列文本细粒度文本提议，每个提议都是文本行的一小部分，它可能包含单个或多个笔画，单个或多个字符，或是字符的一部分。

显然，文本行的垂直位置相对于水平位置是更容易预测的，因此将水平位置固定，来预测其垂直位置会更准确。这与预测物体的4个坐标的RPN相比，减少了搜索空间。

基于以上原因，CTPN采用了垂直锚点机制，可以同时预测每个细粒度提议的文本/非文本分数和y轴坐标。

细粒度的文本提议的具体设计如下。由于使用了VGG16的最后一个卷积特征图，并在其上进行大小为3\*3，步长为1的滑动窗口提取特征。这对于原图像来说，总步长为16像素，因此文本提议的固定宽度被设置为16像素。之后需要设计k个垂直锚点来预测每个提议的y坐标。k个锚点具有相同的水平位置，固定宽度为16像素，但其垂直位置在k个不同的高度变化。作者对每个提议使用十个锚点,k=10,其高度在输入图像中从11个像素变化到273个像素(每次÷0.7)。明确的垂直坐标是通过提议边界框的高度和y轴中心来度量的。计算相对于锚点的边界框位置的相对预测的垂直坐标：

\begin{equation}v_c=(c_y−c^a_y)/h^a,\qquad v_h=log(h/h^a) \tag{1} \end{equation}

\begin{equation}v^∗_c=(c^∗_y−c^a_y)/h^a,\qquad v^∗_h=log(h^∗/h^a) \tag{2} \end{equation}

其中$v=\lbrace v_c,v_h\rbrace$和$v^\*=\lbrace v^\*_c,v^\*_h\rbrace$分别是相对的预测坐标和实际坐标。$c^a_y$和$h^a$是锚盒的中心（y轴）和高度，可以从输入的图像中预先计算。$c_y$和$h$是输入图像中预测的y轴坐标，$c^\*_y$和$h^\*$是实际坐标。一般来说，文本提议框比有效感受野228\*228要小。

由于是在VGG16的最后一层特征图上使用滑动窗口，所以对于每个预测，水平位置（x轴坐标）和k个锚点位置是固定的，可以通过将特征图中的空间窗口位置映射到输入图像上来预先计算。我们的检测器在每个窗口位置输出k个锚点的文本/非文本分数和预测的y轴坐标（v）。检测到的文本提议是从具有>0.7（具有非极大值抑制）的文本/非文本分数的锚点生成的。

通过设计的垂直锚点和细粒度的检测策略，我们的检测器能够通过使用单尺度图像处理各种尺度和长宽比的文本行。这进一步减少了计算量，同时预测了文本行的准确位置。
### 网内循环架构

事实上将每个孤立的提议独立考虑并不鲁棒，可能会导致与文本模式类似的非文本目标（类文本异常值）的误检。还可能丢失一些含有弱文本信息的模糊模式。

收到相关工作的启发，作者认为应用递归神经网络（RNN）来编码用于文本识别的上下文信息对于检测任务也很重要，以便在每个单独的提议中都能做出更可靠的决策。使用RNN的隐藏层对卷积层中的信息直接进行循环编码，形成优雅无缝的网内连接。将每个窗口的卷积特征作为序列输入，并在隐藏层中循环更新内部状态$H_t$

\begin{equation}H_t=\varphi(H_{t-1},X_t)，\qquad t=1,2,...,W \tag{3} \end{equation}

其中$X_t \in R^{3×3×C}$是第t个滑动窗口（3×3）输入的最后一个卷积层特征图的特征，由于滑动窗口是从左向右密集滑动的，因此$t=1，2，...,W$,$W$是最后一个卷积层特征图的宽度。$H_t$是当前输入（$X_t$）和以$H_{t-1}$编码的先前状态联合计算得到的循环内部状态。递归是通过使用非线性函数$\varphi$来计算的，它定义了循环模型的确切形式。

利用LSTM作为RNN层以解决梯度消失问题，并且能获取到之前输入的滑动窗口提取到的特征通过循环连接扫描的序列上下文信息。再通过使用BLSTM扩展RNN层，这样在两个方向上都能对上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如228×W。

内部状态$H_t$被映射到之后的FC层，以及用于计算第t个提议预测的输出层。因此RNN与CNN可以优雅地集成起来，从而形成了一种高效的模型，可以在无需额外成本的情况下进行端到端的训练。

### 文本线构造算法

CTPN可以准确可靠地检测细粒度的文本提议。通过连接其文本/非文本分数>0.7的连续文本提议，可以轻松地构建文本行。文本行构建方法如下，首先，当（i）提议$B_j$和提议$B_i$的水平距离是最接近的，（ii）该水平距离小于50像素，（iii）他们在垂直方向上的重叠比例>0.7，为提议$B_i$定义一个配对邻居（$B_j$）作为$B_j->B_i$。其次$B_j->B_i$并且$B_i->B_j$，则将两个提议组合成一对。然后通过顺序连接具有相同提议的对来构建文本行。

细粒度的检测和RNN连接可以预测垂直方向的精确位置。在水平方向上，图像被分成一系列相等的宽度为16个像素的提议。当两个水平边的文本提议没有完全被实际文本行区域覆盖，或者某些边的提议被丢弃（例如文本得分较低）时，这可能会导致不准确的定位。为了解决这个问题，提出了一种边缘细化的方法，可以精确地估计左右两侧水平方向上的每个锚点/提议的偏移量（称为边缘锚点或边缘提议）。与y坐标预测类似，我们计算相对偏移为：

\begin{equation} o=(x_{side}-c^a_x)/w^a,\qquad o^\*=(x^\*_{side}-c^a_x)/w^a\tag{4} \end{equation}

其中$x_{side}$是最接近的水平边到当前锚点的预测x坐标。$s^\*_{side}$是从实际边界框和锚点位置预先计算的实际边缘的x轴坐标。$c^a_x$是锚点中心的x轴坐标，$w^a$是固定的锚点宽度，这里为固定值16像素。当我们将序列的被检测到的细粒度文本提议连接成文本行时，这些边缘提议将被定义为开始和结束提议。只使用边缘提议的偏移量来优化最终的文本行边界框。模型同时预测了边缘细化的偏移量，不是通过额外的后处理步骤计算的。

### 损失函数的构造

CTPN有三个同时与最后的FC层连接的输出，他们分别预测公式（2）中的文本/非文本分数（s），垂直坐标（$v=\lbrace v_c,v_h\rbrace$）和边缘细化偏移（o）。我们将探索k个锚点来预测它们在VGG16的最后一张特征图中的每个空间位置，从而在输出层分别得到2k,2k,k个参数。

我们采用多任务学习来联合优化模型参数。引入了三种损失函数：$L^{cl}_s,L^{re}_v,L^{re}_o$，分别计算文本/非文本分数，坐标和边缘细化的误差。通过最小化整体目标函数（$L$）进行训练。

\begin{equation}L(s_i, v_j, o_k) =\frac1{N_{s}}\sum_iL^{cl}_{s}(s_i, s_i^\*) +\frac{\lambda_1}{N_v}\sum_j L^{re}_v(v_j, v_j^\*) +\frac{\lambda_2}{N_o}\sum_k L^{re}_o(o_k, o_k^\*) \tag{5} \end{equation}

其中每个锚点都是一个训练样本，i是一个小批量数据中一个锚点的索引。$s_i$是预测的锚点i为真实文本的概率。$s^\*_i=\lbrace 0,1 \rbrace$是真实值。j是y轴坐标回归中有效集合中锚点的索引。有效锚点的定义是正锚点（$s^\*_j=1$）或者与实际文本提议的交并比>0.5。$v_j$和$v^\*_j$分别是第j个锚点的y轴坐标的预测值和真实值。k是边缘锚点的索引，所谓边缘锚点，就是一组的到实际文本行边界框左侧或右侧的水平距离在一定范围内（如32像素）的锚点。$o_k$和$o^\*_k$是第k个锚点x轴上预测的和实际的偏移量。$L^{cl}_s$是分类损失，这里通过使用softmax来区分文本和非文本。$L^{re}_v$和$L^{re}_o$是回归损失，根据先前的工作使用平滑的$L1$函数来计算他们。$\lambda_1$和$\lambda_2$是损失权重，用来平衡不同任务，他们的经验值分别为1.0和2.0。$N_s,N_v$和$N_o$是标准化参数，分别表示$L^{cl}_s,L^{re}_v$和$L^{re}_o$使用的锚点个数。

## 训练与实现细节

通过使用反向传播和随机梯度下降（SGD），可以对CTPN进行端对端训练。与RPN相似，训练的样本是锚点，他们的文职可以在输入的图像中预先计算，因此每个锚点的训练标签可以从相应的实际边框中计算得到。

### 训练标签

对于文本/非文本分类，对于每个正例或反例锚点都分配了一个二值标签。他通过计算与实际边界框的交并比重叠（除以锚点位置）来定义。正例锚点被定义为：(i)锚点与任何的实际边界框具有>0.7的交并比；(ii)与实际边界边框具有最高的交并比。通过条件(ii)，即使是很小的文本模式也能被标记为正锚点。负锚点定义为与所有实际边界框具有<0.5的交并比。y坐标回归($v^\*$)和偏移回归($o^\*$)的训练标签分别按公式(2)和(4)计算。

### 训练数据

训练过程中，每个小批量样本由随机的单张图片组成。每个小批量数据的锚点数量固定为$N_s=128$，正例反例比例为1:1。如果正例的数量少于64，则会使用小图像块填充反例。整个模型使用3000张自然图像进行训练，其中包括来自ICDAR 2013训练集的229张图像。训练使用的图像不与任何测试图像重叠。训练时通过将输入图像的短边设置为600像素来调整输入图像的大小，同时保持原始长宽比。

### 实现细节

使用ImageNet数据预训练VGG16模型，通过使用具有0均值和0.01标准差的高斯分布的随机权重来初始化新层(RNN层和输出层)。模型通过固定前两个卷积层中参数进行端对端的训练。使用0.9的动量和0.0005的权重衰减。在前16K次迭代中，学习率设置为0.001，之后的4k次迭代使用0.0001的学习率。

## 总结

姑且把整篇论文详细看了一遍试着理解一下，但是在一些细节上还是有点一知半解，之后有时间在翻出来复习一下吧。

**ac酱**

**完成于2019-06-09 中午**

> 参考资料
* [Detecting Text in Natural Image with Connectionist Text Proposal Network论文翻译——中英文对照](http://noahsnail.com/2018/02/02/2018-02-02-Detecting%20Text%20in%20Natural%20Image%20with%20Connectionist%20Text%20Proposal%20Network%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E4%B8%AD%E8%8B%B1%E6%96%87%E5%AF%B9%E7%85%A7/)
* [Detecting Text in Natural Image withConnectionist Text Proposal Network](https://arxiv.org/pdf/1609.03605.pdf)
* [CTPN - 自然场景文本检测](https://blog.csdn.net/zchang81/article/details/78873347)
* [场景文字检测—CTPN原理与实现](https://zhuanlan.zhihu.com/p/34757009)
