---
layout: post
title:  "CRNN论文的阅读与学习"
date:   2019-07-29 00:00:00 +0800
categories: cv
tags: CRNN
author: ac酱
mathjax: true
---

* content
{:toc}
最近在项目中使用了CTPN及CRNN进行中文OCR系统的构造，因此很有必要对这两个网络进行更进一步的了解与学习。本文将记录与CRNN相关的内容。



## CRNN流程概括

<center>
<img src="https://raw.githubusercontent.com/changwh/changwh.github.io/master/_posts/res/2019-07-29-crnn-paper-reading/architecture.jpg" />
<div>CRNN网络结构</div>
</center>

<center>
<img src="https://raw.githubusercontent.com/changwh/changwh.github.io/master/_posts/res/2019-07-29-crnn-paper-reading/config_summary.jpg" />
<div>CRNN具体结构</div>
</center>

CRNN的网络结构由三部分组成，分别是卷积层，循环层和转录层。

在CRNN的底部，卷积层自动从每个输入图像中提取特征序列。在卷积网络之上，构建了一个循环网络，用于对卷积层输出的特征序列的每一帧进行预测。采用CRNN顶部的转录层将循环层的每帧预测转化为标签序列。虽然CRNN由不同类型的网络架构（如CNN和RNN）组成，但可以通过一个损失函数进行联合训练。

### 卷积层

采用标准CNN模型（去除全连接层）中的卷积层和最大池化层来构造，用于从输入图像中提取序列特征表示。在进入网络之前，所有的图像需要缩放到相同的高度。然后从卷积层组件产生的特征图中提取特征向量序列作为循环层的输入。特征序列的每一个特征向量在特征图上按列从左到右生成。这意味着第i个特征向量是所有特征图第i列的连接。每列的宽度固定为单个像素。

特征图的每列对应于原始图像的一个矩形区域（称为感受野），并且这些矩形区域与特征图上从左到右的相应列具有相同的顺序。特征序列中的每个向量关联一个感受野，并且可以被认为是该区域的图像描述符。

由于CNN要求将输入图像缩放到固定尺寸，以满足其固定的输入尺寸。而类序列对象的长度变化很大，因此CNN不适合类序列对象。在CRNN中，我们将深度特征传递到序列表示中，以便对类序列对象的长度变化保持适用性的不变。

### 循环层
以深度双向循环神经网络作为循环层，对特征序列$x=x_1,...,x_T$中的每一帧$x_t$都进行预测得到标签分布$y_t$。

这样的循环层具有三个优点：

* 首先，RNN具有很强的捕获序列内上下文信息的能力。对于基于图像的序列识别使用上下文提示比独立处理每个符号更稳定且更有帮助。以场景文本识别为例，宽字符可能需要一些连续的帧来完全描述。此外，一些模糊的字符在观察其上下文时更容易区分，例如，通过对比字符高度更容易识别“il”而不是分别识别它们中的每一个。
* 其次，RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。
* 第三，RNN能够从头到尾对任意长度的序列进行操作。

LSTM是定向的，它只使用过去的上下文。而在基于图像的序列中，两个方向的上下文是相互有用且互补的。因此，我们将两个LSTM，一个向前和一个向后组合到一个双向LSTM中。此外，可以堆叠多个双向LSTM，得到深双向LSTM。深层结构允许比浅层抽象更高层次的抽象，并且在语音识别任务中取得了显著的性能改进。

在循环层中，误差使用BPTT进行反向传播。在循环层的底部，传播差异的序列被连接成图，即将特征图转换为特征序列的操作进行反转，并反馈到卷积层。实际上，我们创建一个称为“Map-to-Sequence”的自定义网络层，作为卷积层和循环层之间的桥梁。

### 转录层

## 特性

* 与大多数现有的组件需要单独训练和协调的算法相比，它是端对端训练的。
* 它自然地处理任意长度的序列，不涉及字符分割或水平尺度归一化。
* 它不仅限于任何预定义的词汇，并且在无词典和基于词典的场景文本识别任务中都取得了显著的表现。
* 它产生了一个有效而小得多的模型，这对于现实世界的应用场景更为实用。

## 独特的优点

* 可以直接从序列标签（例如单词）学习，不需要详细的标注（例如字符）；
* 直接从图像数据学习信息表示时，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；
* 具有与RNN相同的性质，能够产生一系列标签；
* 对类序列对象的长度无约束，只需要在训练阶段和测试阶段对高度进行归一化；
* 它比标准DCNN模型包含的参数要少得多，占用更少的存储空间（不是基于字符组合识别，而是单字符识别，所以分类的数量较少）。

## 关键的IDEA


## 文中提及的相关工作


**to be continued**
